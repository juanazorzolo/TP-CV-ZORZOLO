{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOP6dmJcW/p8/VoycOmWQIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanazorzolo/TP-CV-ZORZOLO/blob/main/TP_CV_ZORZOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRABAJO PRÁCTICO FINAL CV 2025 - SISTEMA DE DETECCIÓN Y CLASIFICACIÓN DE RAZAS DE PERROS**\n",
        "\n",
        "**AUTOR: Juana Zorzolo Rubio (Z-1217/3)**"
      ],
      "metadata": {
        "id": "7_UEWJYbvB1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objetivo General:\n",
        "\n",
        "Desarrollar un pipeline completo de visión por computadora para la identificación de razas de perros en imágenes. El proyecto abarca desde la creación de un sistema de búsqueda por similitud hasta la implementación de un sistema de detección y clasificación en imágenes complejas, incluyendo el entrenamiento y la optimización de modelos de Deep Learning.\n"
      ],
      "metadata": {
        "id": "4H9biV3wveBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 1: Buscador de Imágenes por Similitud"
      ],
      "metadata": {
        "id": "4evSVIMmvoVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación de la Base de Datos Vectorial"
      ],
      "metadata": {
        "id": "-kyvstpywLXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "EUyydzOh0miz",
        "outputId": "679659ef-4758-4c59-d96c-d731e262437b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8758428d-68db-407c-a47c-ac007540706c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8758428d-68db-407c-a47c-ac007540706c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (2).json': b'{\"username\":\"juanazorzolo\",\"key\":\"16e8ca4c7eebb08f98f32ced7754996d\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "4Q1XRNZ107Ab"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d gpiosenka/70-dog-breedsimage-data-set\n",
        "!unzip -q 70-dog-breedsimage-data-set.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD6zMUkr0-kZ",
        "outputId": "e64d6585-16d8-4a3f-f7f9-5a64d39b3f81"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/gpiosenka/70-dog-breedsimage-data-set\n",
            "License(s): CC0-1.0\n",
            "70-dog-breedsimage-data-set.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace /content/dogs.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/test/Afghan/01.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/test/Afghan/02.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/test/Afghan/03.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARACIÓN DEL ENTORNO\n",
        "\n",
        "!pip install -q gradio faiss-cpu torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "import faiss"
      ],
      "metadata": {
        "id": "0FZE1JEcy6zg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las carpetas que existen en /content\n",
        "print(\"Contenido de /content:\")\n",
        "for nombre in os.listdir(\"/content\"):\n",
        "    print(\"-\", nombre)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt9rMYE32ggB",
        "outputId": "077f729d-6044-4a91-8a2a-fd47f7dab8fc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenido de /content:\n",
            "- .config\n",
            "- test\n",
            "- .gradio\n",
            "- train\n",
            "- dogs.csv\n",
            "- kaggle.json\n",
            "- valid\n",
            "- 70-dog-breedsimage-data-set.zip\n",
            "- temp.jpg\n",
            "- kaggle (2).json\n",
            "- kaggle (1).json\n",
            "- sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGAR EL MODELO PREENTRENADO (ResNet 50) (sin la capa final de clasificación)\n",
        "model = resnet50(pretrained=True)\n",
        "model = torch.nn.Sequential(*(list(model.children())[:-1]))  # Quitar la última capa (fc)\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SowvmmQqzMu9",
        "outputId": "f55d5ed3-0598-441b-b178-dbb215ac419e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESAMIENTO DE IMÁGENES\n",
        "\n",
        "# Transformaciones requeridas por ResNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # Imagenet\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "IxfZtexlzO5B"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIÓN PARA EXTRAER LOS EMBEDDINGS\n",
        "\n",
        "def extract_embedding(img_path):\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        embedding = model(img_tensor).squeeze().cpu().numpy()\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "DEYCYhwMzh1g"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESAR EL DATASET Y CONSTRUIR LOS VECTORES\n",
        "\n",
        "# Cargar todas las imágenes y extraer embeddings\n",
        "image_paths = []\n",
        "embeddings = []\n",
        "\n",
        "dataset_paths = [\"/content/train\", \"/content/valid\"] #saque , \"/content/test\" para usarlo después (VER ESTO)\n",
        "image_paths = []\n",
        "embeddings = []\n",
        "\n",
        "for dataset_path in dataset_paths:\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                path = os.path.join(root, file)\n",
        "                try:\n",
        "                    emb = extract_embedding(path)\n",
        "                    image_paths.append(path)\n",
        "                    embeddings.append(emb)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error al procesar {path}: {e}\")\n",
        "\n",
        "embeddings = np.array(embeddings).astype('float32')\n",
        "print(f\"Total imágenes procesadas: {len(embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqwVgd87zoEe",
        "outputId": "eed7f120-cb6f-408d-bd69-ce0c70aaab04"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total imágenes procesadas: 8646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INDEXAR EN BDD VECTORIAL (FAISS)\n",
        "# Crear el índice FAISS\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "H4qa8_791oyi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desarrollo de la Aplicación en Gradio"
      ],
      "metadata": {
        "id": "EME4ROCgwOcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIÓN DE BUSQUEDA EN FAISS\n",
        "def buscar_similares(img_path, k=10):\n",
        "  'toma una imagen, extrae su embedding, busca en FAISS, devuelve las 10 rutas más similares.'\n",
        "  query_emb = extract_embedding(img_path).astype('float32').reshape(1, -1)\n",
        "  distances, indices = index.search(query_emb, k)\n",
        "  resultados = [image_paths[i] for i in indices[0]]\n",
        "  return resultados"
      ],
      "metadata": {
        "id": "MnVINzwq3v8K"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INTERFAZ EN GRADIO\n",
        "import gradio as gr\n",
        "def interfaz_gradio(imagen):\n",
        "    # Guardar la imagen temporalmente\n",
        "    temp_path = \"/content/temp.jpg\"\n",
        "    imagen.save(temp_path)\n",
        "\n",
        "    # Buscar similares\n",
        "    similares = buscar_similares(temp_path)\n",
        "\n",
        "    # Cargar las imágenes similares\n",
        "    resultados = [Image.open(p) for p in similares]\n",
        "\n",
        "    return resultados"
      ],
      "metadata": {
        "id": "0VZGVzjD4IE0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lanzar interfaz\n",
        "gr.Interface(\n",
        "    fn=interfaz_gradio,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Image(type=\"pil\", label=f\"Similar #{i+1}\") for i in range(10)],\n",
        "    title=\"Buscador de Razas de Perros por Similitud\",\n",
        "    description=\"Subí una imagen de un perro y te mostramos las 10 más similares del dataset.\"\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "JnL3_o5C4UG-",
        "outputId": "281c8f3d-c44f-4590-d5ac-58f0ed7936ab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://cc0542b2a2f1ce36e9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cc0542b2a2f1ce36e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1895: UserWarning: A function (interfaz_gradio) returned too many output values (needed: 10, returned: 12). Ignoring extra values.\n",
            "    Output components:\n",
            "        [image, image, image, image, image, image, image, image, image, image]\n",
            "    Output values returned:\n",
            "        [<PIL.Image.Image image mode=RGB size=612x512 at 0x7D7D02AABE90>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D021C9590>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D021CAA50>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D021E0150>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D021E1490>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D035BECD0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D03594B90>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D021CFE90>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D02CA5C50>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D03588450>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D7D03588110>, \"Raza predicha: Golden Retriever\"]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cc0542b2a2f1ce36e9.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificación Basada en Similitud y Métrica de Evaluación"
      ],
      "metadata": {
        "id": "ofVpkICtwSZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARA CLASIFICACIÓN POR VOTO MAYORITARIO, ES NECESARIO EXTRAER LA RAZA REAL DESDE EL PATH DE CADA IMAGEN\n",
        "# como en el dataset las carpetas se llaman como la raza\n",
        "\n",
        "def extraer_raza(path):\n",
        "    return os.path.basename(os.path.dirname(path))"
      ],
      "metadata": {
        "id": "rctJLFsM5_pA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en interfaz_gradio, se agrega el voto mayoritario sobre las razas de las 10 imágenes recuperadas\n",
        "from collections import Counter\n",
        "\n",
        "def interfaz_gradio(imagen):\n",
        "    temp_path = \"/content/temp.jpg\"\n",
        "    imagen.save(temp_path)\n",
        "\n",
        "    similares = buscar_similares(temp_path)\n",
        "    resultados = [Image.open(p) for p in similares]\n",
        "\n",
        "    # Voto mayoritario\n",
        "    razas = [extraer_raza(p) for p in similares]\n",
        "    raza_predicha = Counter(razas).most_common(1)[0][0]\n",
        "\n",
        "    return [imagen] + resultados + [f\"Raza predicha: {raza_predicha}\"]"
      ],
      "metadata": {
        "id": "iFu6xwvt6TT5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cambio la interfaz para que también muestre la imagen subida y el texto\n",
        "\n",
        "gr.Interface(\n",
        "    fn=interfaz_gradio,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Image(type=\"pil\", label=\"Imagen de entrada\")] +\n",
        "            [gr.Image(type=\"pil\", label=f\"Similar #{i+1}\") for i in range(10)] +\n",
        "            [gr.Textbox(label=\"Raza Predicha\")],\n",
        "    title=\"Buscador de Razas de Perros por Similitud\",\n",
        "    description=\"Subí una imagen de un perro y te mostramos las 10 más similares del dataset y la raza más probable.\"\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "collapsed": true,
        "id": "jy5gTnw86aH7",
        "outputId": "894e960c-915b-4143-9357-62ad43b33a4a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://cc8ec2dd0fe7abbe1b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cc8ec2dd0fe7abbe1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cc8ec2dd0fe7abbe1b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Cálculo de NDCG@10\n",
        "\n",
        "def extraer_raza(path):\n",
        "    return os.path.basename(os.path.dirname(path))\n",
        "\n",
        "def dcg(relevancias):\n",
        "    return sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevancias))\n",
        "\n",
        "def ndcg(relevancias):\n",
        "    ideal = sorted(relevancias, reverse=True)\n",
        "    return dcg(relevancias) / (dcg(ideal) + 1e-8)\n",
        "\n",
        "def evaluar_ndcg(test_images, k=10):\n",
        "    scores = []\n",
        "\n",
        "    for img_path in tqdm(test_images):\n",
        "        true_raza = extraer_raza(img_path)\n",
        "        similares = buscar_similares(img_path, k=k)\n",
        "        razas_similares = [extraer_raza(p) for p in similares]\n",
        "\n",
        "        relevancias = [1 if raza == true_raza else 0 for raza in razas_similares]\n",
        "        score = ndcg(relevancias)\n",
        "        scores.append(score)\n",
        "\n",
        "    ndcg_promedio = np.mean(scores)\n",
        "    print(f\"NDCG@{k} promedio: {ndcg_promedio:.4f}\")"
      ],
      "metadata": {
        "id": "nTSqO1yL7OVL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: usar 5 imágenes de prueba por raza\n",
        "from collections import defaultdict\n",
        "\n",
        "test_set = []\n",
        "razas_vistas = defaultdict(int)\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/test\"):\n",
        "    for file in files:\n",
        "        if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            raza = os.path.basename(root)\n",
        "            if razas_vistas[raza] < 5:\n",
        "                test_set.append(os.path.join(root, file))\n",
        "                razas_vistas[raza] += 1\n",
        "\n",
        "print(f\"Imágenes de prueba: {len(test_set)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdFKOLNS7Qxj",
        "outputId": "0151fe77-636b-496a-8c7b-bb9d03c16d9c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imágenes de prueba: 350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_ndcg(test_set, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvzZAMxo8BQq",
        "outputId": "ca4c04a0-f691-4d04-cf7a-0f99e1d7f90d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 350/350 [00:05<00:00, 59.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG@10 promedio: 0.9733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 2: Entrenamiento y Comparación de Modelos de Clasificación"
      ],
      "metadata": {
        "id": "awypW0Lyvp2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de Modelos"
      ],
      "metadata": {
        "id": "RTTumFPrv3nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo A (Transfer Learning)"
      ],
      "metadata": {
        "id": "L5Lbb7cEv6JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo B (Opcional, recomendado)"
      ],
      "metadata": {
        "id": "VwUT1zx5v9Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integración y Selección en la Aplicación"
      ],
      "metadata": {
        "id": "tiT0fEc_wBxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 3: Pipeline de Detección y Clasificación en Escenas Complejas"
      ],
      "metadata": {
        "id": "xZZV2AkIvs8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detección de Objetos"
      ],
      "metadata": {
        "id": "fwLVYNtrwWFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del Pipeline Completo"
      ],
      "metadata": {
        "id": "uWcSa8OpwYi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 4: Evaluación, Optimización y Herramientas de Anotación"
      ],
      "metadata": {
        "id": "Cg2AOo46vwop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del Pipeline"
      ],
      "metadata": {
        "id": "Wp812DKQwbke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimización de Modelos (Elegir una)"
      ],
      "metadata": {
        "id": "FScdk2l_wda-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Script de Anotación Automática"
      ],
      "metadata": {
        "id": "rGaNqsKrwgKI"
      }
    }
  ]
}